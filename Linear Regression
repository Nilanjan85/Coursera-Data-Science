Gradient Descent loop implementation with R

# Generate random data
> x <- runif(500, -4, 4)  
> y <- x + rnorm(500) + 2.5 
# Define the squared error cost function    
> cost <- function(X, y, theta) {
> sum( (X %*% theta - y)^2 ) / (2*length(y))
> }
> alpha <- 0.1 # Specify the learning rate
> num_iters <- 1000 # Specify the number of iterations
> cost_history <- rep(0,num_iters) # will be used to store the value of cost function after
                                   # every iteration 
> theta_history <- list(num_iters) # will be used to store the value of theta after every 
                                   # iteration 
> theta <-  c(0,0) # Initial values of theta
> X <- cbind(1,x) # Add a column vector with all values  to be 1 to x so that hypothesis 
                  # function has an intercept 
> for (i in 1:num_iters) {
>   theta[1] <- theta[1] - alpha * (1/length(y)) * sum(((X%*%theta)- y))
>   theta[2] <- theta[2] - alpha * (1/length(y)) * sum(((X%*%theta)- y)*X[,2])
>   cost_history[i] <- cost(X, y, theta)
>   theta_history[[i]] <- theta
> } 
> print(theta)
[1] 2.470261 1.020998
# Plots the training dataset
> plot(x,y, col=rgb(0.2,0.4,0.6,0.4), main='Linear regression by gradient descent')
# Plots various lines during the course of convergence
> for (i in c(1,3,6,10,14,seq(20,num_iters,by=10))) {
>  abline(coef=theta_history[[i]], col=rgb(0.8,0,0,0.3))
> }
> abline(coef=theta, col='blue') # Plots a straight line with intercept as theta[1] and slope
                                 # as theta[2]
                                


##---- Linear regression in one variable(linear regression)-------

##-----------Loading Data------------------------- 
data <- read.table("ex1data2.txt", sep=',')
x<- data[,1:2]
y<- data$V2
len<- dim(data)[1]

##--------Feature Normalization --------------

featureNormalize <- function(x){

mean_val1 <- mean(x[,1])
mean_val2 <- mean(x[,2])

sd_val1 <- sd(x[,1])
sd_val2 <- sd(x[,2])

x[,1] <- (x[,1]-mean_val1)/sd_val1
x[,2] <- (x[,2]-mean_val2)/sd_val2

return(x)

}

x <- featureNormalize(x)

## ------------------- Part 3: COST & Gradient descent -------------------



## Add a column of 1's to the feature X and make it a matrix. 
x<-cbind(rep(1,len),x)
x<- as.matrix(x)

## initialize fitting parameters
theta <- rep(0,3)

# compute and display initial cost
costComp <-  function(x,y,theta,len){
J <- 0
dif <- x %*% theta - y
J <- (t(dif) %*% dif) / (2 * len)
return(J)
}


#compute gradient descent
gradDescent <- function(x,y,theta,alpha,iteration)
{
    J_hist <- rep(0,iteration)

for (i in 1:iteration)
{
    theta<- theta - alpha*(1/len)*(t(x)%*%(x%*%theta-y))
    J_hist[i] <- costComp(x,y,theta,len)
}
    result <- list(theta,J_hist)
    return(result)
}

# calling gradient descent
iteration<- 1500
alpha <- 0.01 ## learning rate

results <- gradDescent(x,y,theta,alpha,iteration)
theta_result <- results[[1]]
cost_hist_result <- results[[2]]

print("The value of the final theta ")
print(theta_result)

##print("The values of the cost function")
##print(cost_hist_result)

## Plotting the cost function J(Theta) value

plot(1:iteration,cost_hist_result,type ='l') 

# ploting the linear regression line
lines(x[, 2], x  %*% theta_result, col="blue")

#####------Below predction is for univariate linear regression -----------

#Predicting vales for 35,000 and 70,000
predict1 <- c(1,3.5) %*% theta_result
print("the predictate value for 35,000 is =")
print(predict1*1000)

predict2 <- c(1,7) %*% theta_result
print("the predictate value for 70,000 is =")
print(predict2*1000)









##---- Linear regression in one variable(linear regression)-------

##-----------Loading Data------------------------- 
data <- read.table("ex1data2.txt", sep=',')
x<- data[,1:2]
y<- data$V2
len<- dim(data)[1]

##--------Feature Normalization --------------

featureNormalize <- function(x){

mean_val1 <- mean(x[,1])
mean_val2 <- mean(x[,2])

sd_val1 <- sd(x[,1])
sd_val2 <- sd(x[,2])

x[,1] <- (x[,1]-mean_val1)/sd_val1
x[,2] <- (x[,2]-mean_val2)/sd_val2

return(x)

}

x <- featureNormalize(x)

## ------------------- Part 3: COST & Gradient descent -------------------



## Add a column of 1's to the feature X and make it a matrix. 
x<-cbind(rep(1,len),x)
x<- as.matrix(x)

## initialize fitting parameters
theta <- rep(0,3)

# compute and display initial cost
costComp <-  function(x,y,theta,len){
J <- 0
dif <- x %*% theta - y
J <- (t(dif) %*% dif) / (2 * len)
return(J)
}


#compute gradient descent
gradDescent <- function(x,y,theta,alpha,iteration)
{
    J_hist <- rep(0,iteration)

for (i in 1:iteration)
{
    theta<- theta - alpha*(1/len)*(t(x)%*%(x%*%theta-y))
    J_hist[i] <- costComp(x,y,theta,len)
}
    result <- list(theta,J_hist)
    return(result)
}

# calling gradient descent
iteration<- 1500
alpha <- 0.01 ## learning rate

results <- gradDescent(x,y,theta,alpha,iteration)
theta_result <- results[[1]]
cost_hist_result <- results[[2]]

print("The value of the final theta ")
print(theta_result)

##print("The values of the cost function")
##print(cost_hist_result)

## Plotting the cost function J(Theta) value

plot(1:iteration,cost_hist_result,type ='l') 

# ploting the linear regression line
lines(x[, 2], x  %*% theta_result, col="blue")

#####------Below predction is for univariate linear regression -----------

#Predicting vales for 35,000 and 70,000
predict1 <- c(1,3.5) %*% theta_result
print("the predictate value for 35,000 is =")
print(predict1*1000)

predict2 <- c(1,7) %*% theta_result
print("the predictate value for 70,000 is =")
print(predict2*1000)








## Loding data
print("Loading Data")

data <- read.table("ex1data2.txt", header=FALSE,sep=',')

x <- data[,1:2]
y <- data[,3] 
len <- length(x)

print("printing the data")

head(x)
head(y)

## calling the function to normalize feature
x <- featureNormalize(x)

head(x)

featureNormalize <- function(x){

mean_val1 <- mean(x[,1])
mean_val2 <- mean(x[,2])

sd_val1 <- sd(x[,1])
sd_val2 <- sd(x[,2])

x[,1] <- (x[,1]-mean_val1)/sd_val1
x[,2] <- (x[,2]-mean_val2)/sd_val2

return(x)

}











#install.packages("MASS")
library("MASS")

NormalEquation <- function(x,y){
    
x <- as.matrix(x)
temp <- t(x) %*% x
theta <- ginv(temp) %*% t(x) %*% y     

return(theta)

}

#===========Normal Equation===============
print("Solving with Normal Equation")

## Load Data

d1 <- read.table("ex1data2.txt",sep=",",header=FALSE)

x <- d1[,1:2]
y <- d1[,3]
len <- length(y)

##Add intercept term to X

x <- cbind(rep(1,len),x)

##Calculate the parameters from the normal equation

theta <- NormalEquation(x,y)

##Display normal equation's result
 
cat("Theta calculated from the normal equation")

print(theta)

## Estimate the price of a 1650 sq-ft, 3 br house

price <- c(1,1650,3) %*% theta

print("predicted house price by normal equation")
print(price)










